{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rToK0Tku8PPn"
      },
      "source": [
        "## makemore: becoming a backprop ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sFElPqq8PPp"
      },
      "outputs": [],
      "source": [
        "# there no change change in the first several cells from last lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ChBbac4y8PPq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klmu3ZG08PPr",
        "outputId": "19534265-511f-469b-e8b1-df5d66a81ad5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read in all the words\n",
        "words = open('../2 - makemore/names.txt', 'r').read().splitlines()\n",
        "words[:8]\n",
        "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCQomLE_8PPs",
        "outputId": "0a35e2f3-42aa-4f94-a4a0-ba44d08a3592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_zt2QHr8PPs",
        "outputId": "a10d0377-3297-4464-8a7e-530b092e2e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):  \n",
        "  X, Y = [], []\n",
        "  \n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eg20-vsg8PPt"
      },
      "outputs": [],
      "source": [
        "# ok biolerplate done, now we get to the action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MJPU8HT08PPu"
      },
      "outputs": [],
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlFLjQyT8PPu",
        "outputId": "18e4c31c-f708-4571-b052-e46514013e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4137\n"
          ]
        }
      ],
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QY-y96Y48PPv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ofj1s6d8PPv",
        "outputId": "8ac7ee2b-2d1a-4a51-bd3a-ae4bf26449dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(3.3505, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani # x-u\n",
        "bndiff2 = bndiff**2 \n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n) || variance, sigma^2\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv # x_hat\n",
        "hpreact = bngain * bnraw + bnbias # y\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv ## just normalising, counts / counts.sum(1, keepdims=True)\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEzNtq0qteBj",
        "outputId": "6ed691b9-f033-412d-844b-eb025c944a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-2.5823, -2.4418, -3.9060, -2.9445, -3.8890, -2.4887, -3.8025, -3.2968,\n",
            "         -4.0800, -3.4857, -3.3812, -3.2194, -3.2987, -3.5811, -3.3887, -4.2905,\n",
            "         -4.6760, -4.0194, -4.1200, -2.9517, -2.8796, -3.8509, -3.6637, -2.6481,\n",
            "         -2.9210, -3.7308, -3.8813],\n",
            "        [-2.8827, -2.8315, -2.3772, -2.8874, -3.3825, -3.3425, -4.0167, -3.1033,\n",
            "         -3.9158, -3.7564, -3.0263, -3.0858, -3.0230, -3.5847, -3.1128, -3.2339,\n",
            "         -3.7269, -4.0585, -3.8210, -3.2742, -3.9461, -3.7605, -4.1638, -2.7460,\n",
            "         -3.7331, -3.3271, -3.7580],\n",
            "        [-3.9332, -3.7268, -4.2446, -4.2858, -3.7491, -3.0247, -2.8519, -2.7813,\n",
            "         -2.9234, -3.4812, -3.9108, -3.3292, -3.1411, -3.1314, -3.7879, -3.6449,\n",
            "         -4.2508, -3.3705, -3.4712, -2.2120, -2.6279, -3.3457, -3.2258, -3.2448,\n",
            "         -3.2663, -3.9618, -3.5794],\n",
            "        [-3.3769, -3.5979, -3.1605, -2.9654, -2.8066, -3.4942, -3.0652, -3.2225,\n",
            "         -3.0245, -4.0125, -3.2722, -3.5754, -3.4202, -3.1971, -2.8591, -2.7222,\n",
            "         -3.6691, -3.6513, -4.0285, -2.9744, -4.0198, -3.6902, -3.2669, -3.8311,\n",
            "         -3.6063, -3.0326, -3.2292],\n",
            "        [-4.1298, -4.1600, -3.7549, -3.7407, -3.9860, -3.1535, -3.3064, -4.0336,\n",
            "         -2.9239, -3.2769, -3.3932, -4.1468, -3.2621, -3.7488, -3.0476, -1.8038,\n",
            "         -3.9566, -2.9832, -2.9113, -2.7849, -3.1619, -4.0861, -3.8339, -4.1797,\n",
            "         -3.1053, -3.7669, -3.2006],\n",
            "        [-3.2316, -3.2268, -2.9644, -2.7440, -2.7145, -3.5567, -3.5127, -2.9781,\n",
            "         -3.6318, -4.1278, -3.1059, -3.8357, -3.6013, -3.1519, -2.7792, -3.0236,\n",
            "         -4.0670, -3.8593, -4.5265, -3.5619, -3.4610, -3.5064, -3.1254, -3.9804,\n",
            "         -4.0657, -2.5081, -3.2260],\n",
            "        [-2.8485, -4.4494, -2.7608, -4.1834, -3.5099, -3.7831, -2.7568, -2.9310,\n",
            "         -3.0643, -3.1386, -4.0570, -3.2456, -3.6132, -4.0039, -4.0824, -2.6750,\n",
            "         -2.5780, -3.1183, -4.5324, -3.4260, -3.5911, -2.9311, -3.1366, -3.7858,\n",
            "         -3.1725, -3.6923, -3.5804],\n",
            "        [-2.9640, -3.3571, -3.7197, -2.1146, -3.6426, -2.8309, -3.4560, -2.5572,\n",
            "         -3.3709, -4.6924, -3.5777, -4.1055, -3.9993, -2.9213, -3.9707, -4.2070,\n",
            "         -4.2758, -4.2684, -4.0964, -2.5193, -3.3899, -3.0011, -3.6852, -3.2780,\n",
            "         -3.7208, -2.9188, -3.4737],\n",
            "        [-3.0256, -3.4814, -4.0457, -3.2602, -4.5282, -3.0842, -3.0334, -2.5029,\n",
            "         -3.6082, -3.7980, -3.5347, -3.3905, -3.2909, -3.7658, -4.4957, -3.9985,\n",
            "         -3.9962, -3.2768, -3.7659, -2.4077, -3.2397, -2.4330, -3.1670, -3.1202,\n",
            "         -3.1045, -4.1076, -3.4359],\n",
            "        [-3.5127, -4.2837, -3.5639, -3.9223, -3.4295, -3.1631, -3.4315, -3.4687,\n",
            "         -2.4982, -2.8158, -4.3324, -3.2578, -3.2829, -3.1644, -3.6720, -3.6159,\n",
            "         -2.8513, -3.0356, -3.1103, -3.7193, -2.6487, -3.6697, -2.6987, -3.9793,\n",
            "         -3.4437, -3.2499, -4.0921],\n",
            "        [-4.1646, -4.8590, -3.2915, -3.1108, -3.3962, -3.2632, -4.0177, -5.0365,\n",
            "         -3.3321, -3.0951, -3.6931, -3.9266, -2.9000, -3.5341, -3.1886, -1.9526,\n",
            "         -3.0129, -2.8711, -2.4426, -3.3431, -3.1102, -4.6294, -4.3876, -3.7537,\n",
            "         -3.2782, -3.6094, -3.7837],\n",
            "        [-3.7364, -3.4239, -3.7807, -1.6542, -4.4524, -2.8282, -2.7531, -3.0793,\n",
            "         -3.2161, -3.4809, -4.1962, -3.9449, -3.9668, -3.6289, -5.0155, -4.9627,\n",
            "         -3.8693, -3.3625, -4.0909, -2.9014, -2.9539, -3.1669, -3.9662, -3.2557,\n",
            "         -3.0343, -3.1554, -4.2044],\n",
            "        [-3.8356, -4.3195, -3.1127, -4.1926, -3.6271, -3.9780, -2.5079, -3.2779,\n",
            "         -2.9209, -3.0083, -4.0952, -3.1828, -3.2687, -3.5938, -4.1276, -3.2596,\n",
            "         -2.6020, -3.0499, -3.1059, -3.8141, -2.9996, -3.2610, -2.3111, -4.5509,\n",
            "         -3.7823, -3.6757, -3.6768],\n",
            "        [-3.7714, -3.5915, -2.9480, -3.8226, -4.0048, -2.5740, -3.5796, -4.1432,\n",
            "         -3.5788, -3.0973, -3.5491, -3.3704, -2.6974, -3.3544, -2.9825, -3.1349,\n",
            "         -3.5752, -3.4023, -2.4378, -4.2492, -2.7431, -4.1622, -4.0009, -3.3339,\n",
            "         -3.3527, -3.2598, -3.3819],\n",
            "        [-3.0105, -3.6899, -3.5497, -2.8847, -4.0252, -3.2271, -3.5349, -2.8847,\n",
            "         -3.0313, -3.0359, -3.4494, -3.0836, -3.0135, -3.3443, -3.6087, -3.7112,\n",
            "         -3.8408, -2.9099, -3.4719, -2.9574, -2.8470, -3.0683, -3.6268, -3.3624,\n",
            "         -3.8155, -3.8681, -3.8567],\n",
            "        [-3.1200, -2.8424, -3.4864, -3.9525, -3.8896, -2.4943, -3.5498, -3.7257,\n",
            "         -3.6781, -2.9479, -3.3686, -3.0226, -2.8374, -4.1547, -3.4860, -4.0617,\n",
            "         -3.2476, -3.8988, -2.7370, -4.0343, -3.1992, -3.6329, -3.4155, -2.7263,\n",
            "         -2.9089, -3.5795, -3.9434],\n",
            "        [-3.4977, -3.6850, -2.4542, -3.1137, -2.8009, -2.9368, -4.1216, -3.7217,\n",
            "         -3.9129, -3.9848, -3.0022, -3.8248, -3.1931, -2.9060, -2.6577, -2.7948,\n",
            "         -4.2023, -4.0303, -3.6936, -3.1119, -3.6067, -4.0194, -4.6219, -3.0282,\n",
            "         -3.7662, -3.0901, -3.0180],\n",
            "        [-2.8919, -2.4800, -3.0977, -2.8891, -3.2008, -3.3139, -3.8064, -2.6826,\n",
            "         -3.7458, -4.0253, -2.6654, -4.2054, -3.7770, -4.0229, -3.1343, -3.5557,\n",
            "         -4.1899, -3.4839, -3.9676, -2.9968, -3.7614, -3.0875, -3.6906, -3.8434,\n",
            "         -3.6550, -3.1677, -2.8908],\n",
            "        [-3.8356, -4.3195, -3.1127, -4.1926, -3.6271, -3.9780, -2.5079, -3.2779,\n",
            "         -2.9209, -3.0083, -4.0952, -3.1828, -3.2687, -3.5938, -4.1276, -3.2596,\n",
            "         -2.6020, -3.0499, -3.1059, -3.8141, -2.9996, -3.2610, -2.3111, -4.5509,\n",
            "         -3.7823, -3.6757, -3.6768],\n",
            "        [-3.7003, -3.8441, -3.3720, -4.0779, -3.2576, -3.2442, -2.8059, -3.0348,\n",
            "         -3.4482, -3.3330, -3.5909, -3.0498, -2.8229, -3.0012, -4.1722, -3.6810,\n",
            "         -3.5800, -3.8980, -3.3766, -2.4854, -2.8976, -3.3994, -3.1702, -2.9921,\n",
            "         -3.4178, -3.8648, -3.7172],\n",
            "        [-2.9304, -4.1996, -4.0313, -3.1931, -3.4965, -3.2272, -3.5394, -3.3023,\n",
            "         -2.9044, -3.8394, -2.7711, -3.3229, -2.8412, -3.8710, -3.1698, -2.6061,\n",
            "         -4.1093, -3.7816, -3.2292, -2.8919, -3.9310, -3.4450, -3.7306, -3.6337,\n",
            "         -3.2722, -2.8586, -3.3352],\n",
            "        [-2.8881, -2.6565, -2.9965, -3.0018, -3.6387, -2.8274, -3.4149, -3.1127,\n",
            "         -3.7236, -4.1145, -2.9612, -3.6012, -3.7707, -3.5097, -3.1818, -3.1423,\n",
            "         -3.9590, -3.2365, -3.8758, -2.4070, -4.3229, -3.4470, -4.1997, -3.5021,\n",
            "         -3.4223, -3.4948, -3.4401],\n",
            "        [-3.8356, -4.3195, -3.1127, -4.1926, -3.6271, -3.9780, -2.5079, -3.2779,\n",
            "         -2.9209, -3.0083, -4.0952, -3.1828, -3.2687, -3.5938, -4.1276, -3.2596,\n",
            "         -2.6020, -3.0499, -3.1059, -3.8141, -2.9996, -3.2610, -2.3111, -4.5509,\n",
            "         -3.7823, -3.6757, -3.6768],\n",
            "        [-3.0366, -4.0140, -3.6566, -4.3992, -2.8722, -3.5583, -2.8263, -3.4569,\n",
            "         -3.7615, -3.2210, -3.4927, -3.2022, -3.3919, -3.3752, -3.2738, -3.0640,\n",
            "         -2.6235, -3.7451, -3.2353, -4.1296, -3.9887, -3.7422, -2.1375, -4.0183,\n",
            "         -2.8535, -3.4990, -4.1546],\n",
            "        [-3.3750, -2.5529, -3.8226, -3.0395, -3.6164, -2.6998, -3.6908, -3.5558,\n",
            "         -3.6435, -3.5861, -2.3446, -4.0949, -3.4679, -4.4550, -3.2891, -3.9615,\n",
            "         -4.1100, -4.0952, -4.3317, -3.5854, -4.2628, -3.7134, -4.4582, -1.8395,\n",
            "         -2.9892, -3.6458, -3.0850],\n",
            "        [-2.7694, -3.6223, -4.2421, -3.8520, -3.4136, -2.3846, -3.2799, -3.4035,\n",
            "         -3.7264, -3.3770, -3.5128, -2.9726, -2.9512, -3.3894, -3.8986, -4.3342,\n",
            "         -3.4521, -3.8574, -2.8390, -4.2535, -3.6503, -3.8101, -2.6477, -2.9619,\n",
            "         -2.6350, -3.1614, -4.5870],\n",
            "        [-3.1480, -3.7608, -3.8847, -4.5860, -2.9990, -3.7088, -2.5536, -3.1943,\n",
            "         -3.6237, -2.9796, -4.0327, -2.9936, -2.8928, -3.5303, -3.5716, -3.9368,\n",
            "         -2.8869, -3.6078, -3.3916, -3.4124, -3.8028, -3.3568, -2.0408, -3.9749,\n",
            "         -3.1787, -3.6588, -4.6559],\n",
            "        [-3.5365, -3.5940, -3.0521, -3.8694, -3.3002, -3.6692, -3.3020, -3.5318,\n",
            "         -3.3418, -2.9893, -3.8874, -3.4901, -3.3031, -3.3429, -3.0832, -2.8016,\n",
            "         -3.1086, -3.0289, -2.9536, -4.1188, -2.3044, -4.1957, -2.9961, -4.4371,\n",
            "         -4.0741, -3.3584, -3.1801],\n",
            "        [-2.7177, -3.2664, -4.1979, -3.8374, -3.3341, -2.6990, -3.2383, -3.2154,\n",
            "         -4.6426, -3.6584, -3.1375, -3.5406, -3.7356, -3.8955, -3.5929, -3.5139,\n",
            "         -2.6568, -3.6807, -2.6670, -3.6796, -3.3250, -3.4214, -3.0740, -2.8794,\n",
            "         -2.5721, -4.4035, -4.0202],\n",
            "        [-3.7539, -4.0468, -3.4567, -3.4159, -2.9691, -3.7052, -2.7688, -3.4579,\n",
            "         -2.6689, -3.4527, -3.5715, -3.2157, -3.5068, -3.3253, -3.4536, -2.7346,\n",
            "         -3.5214, -3.5351, -3.2328, -3.5862, -3.1422, -3.5531, -2.8207, -4.2405,\n",
            "         -3.3809, -2.9070, -3.4264],\n",
            "        [-3.2730, -2.9346, -3.8248, -2.7253, -3.0863, -2.9592, -3.3298, -3.5293,\n",
            "         -3.6550, -3.8881, -3.3604, -3.7545, -4.1107, -3.7107, -4.5003, -3.4435,\n",
            "         -3.3712, -3.4836, -3.0824, -3.0309, -2.8522, -3.5773, -3.8536, -2.5915,\n",
            "         -2.4670, -3.8646, -3.7385],\n",
            "        [-3.0334, -3.7420, -2.7979, -3.9696, -2.5908, -3.5060, -4.1205, -3.7501,\n",
            "         -4.2005, -2.9357, -3.5925, -3.4352, -3.4297, -2.5275, -3.1212, -3.3223,\n",
            "         -2.9873, -3.1388, -3.1016, -4.3204, -2.8848, -4.4973, -2.7204, -3.8047,\n",
            "         -3.9716, -3.2339, -4.0214]], grad_fn=<LogBackward0>)\n"
          ]
        }
      ],
      "source": [
        "logprobs.shape\n",
        "print(logprobs)\n",
        "# Remember how the loss thing works.\n",
        "# -ve, logprobs[range(n), Yb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
            "        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])\n",
            "tensor([-4.0800, -3.1128, -3.6449, -3.2669, -4.1298, -3.5619, -3.1386, -3.9707,\n",
            "        -3.0842, -4.2837, -3.1102, -1.6542, -2.9209, -2.9825, -3.0135, -3.1200,\n",
            "        -3.8248, -2.8919, -3.6768, -3.3330, -2.8586, -2.8881, -4.3195, -4.0140,\n",
            "        -3.5558, -2.8390, -2.9796, -3.8694, -2.6990, -3.4527, -3.2730, -3.1016],\n",
            "       grad_fn=<IndexBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(Yb)\n",
        "## All this bit is doing is going down each row, and then selecting the Yb'th column from that row, and then returning that\n",
        "print(logprobs[range(n), Yb]) #Right?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# And then our loss is just meaning the above, and then -ve it. \n",
        "# e.g loss = - (a+b+c)/3 = -1/3a + -1/3b + -1/3c for a 3 example thing sooo that means\n",
        "# something like dloss/da = -1/3, etc... I.e it'd all be -1/3 for that 3 num example.\n",
        "# So that means dloss/da (or any element in our selected logprobs this case) would be -1/num(elements in logprobs[range(n), Yb]) = -1/n in this case\n",
        "# And intuitively, the logprobs not in the selected range would have a 0 gradient because changing them wouldn't affect the loss :P (in this batch, so they have a 0 gradient here)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, so dprobs... dL/dprobs, right? Well, we'll be doing the gradients of the above * the our local derivative right?\n",
        "And we're logging everything from probs.\n",
        "So it'll just be 1/probs * dlogprobs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-2.5823, -2.4418, -3.9060, -2.9445, -3.8890, -2.4887, -3.8025, -3.2968,\n",
              "         -4.0800, -3.4857, -3.3812, -3.2194, -3.2987, -3.5811, -3.3887, -4.2905,\n",
              "         -4.6760, -4.0194, -4.1200, -2.9517, -2.8796, -3.8509, -3.6637, -2.6481,\n",
              "         -2.9210, -3.7308, -3.8813],\n",
              "        [-2.8827, -2.8315, -2.3772, -2.8874, -3.3825, -3.3425, -4.0167, -3.1033,\n",
              "         -3.9158, -3.7564, -3.0263, -3.0858, -3.0230, -3.5847, -3.1128, -3.2339,\n",
              "         -3.7269, -4.0585, -3.8210, -3.2742, -3.9461, -3.7605, -4.1638, -2.7460,\n",
              "         -3.7331, -3.3271, -3.7580],\n",
              "        [-3.9332, -3.7268, -4.2446, -4.2858, -3.7491, -3.0247, -2.8519, -2.7813,\n",
              "         -2.9234, -3.4812, -3.9108, -3.3292, -3.1411, -3.1314, -3.7879, -3.6449,\n",
              "         -4.2508, -3.3705, -3.4712, -2.2120, -2.6279, -3.3457, -3.2258, -3.2448,\n",
              "         -3.2663, -3.9618, -3.5794],\n",
              "        [-3.3769, -3.5979, -3.1605, -2.9654, -2.8066, -3.4942, -3.0652, -3.2225,\n",
              "         -3.0245, -4.0125, -3.2722, -3.5754, -3.4202, -3.1971, -2.8591, -2.7222,\n",
              "         -3.6691, -3.6513, -4.0285, -2.9744, -4.0198, -3.6902, -3.2669, -3.8311,\n",
              "         -3.6063, -3.0326, -3.2292],\n",
              "        [-4.1298, -4.1600, -3.7549, -3.7407, -3.9860, -3.1535, -3.3064, -4.0336,\n",
              "         -2.9239, -3.2769, -3.3932, -4.1468, -3.2621, -3.7488, -3.0476, -1.8038,\n",
              "         -3.9566, -2.9832, -2.9113, -2.7849, -3.1619, -4.0861, -3.8339, -4.1797,\n",
              "         -3.1053, -3.7669, -3.2006],\n",
              "        [-3.2316, -3.2268, -2.9644, -2.7440, -2.7145, -3.5567, -3.5127, -2.9781,\n",
              "         -3.6318, -4.1278, -3.1059, -3.8357, -3.6013, -3.1519, -2.7792, -3.0236,\n",
              "         -4.0670, -3.8593, -4.5265, -3.5619, -3.4610, -3.5064, -3.1254, -3.9804,\n",
              "         -4.0657, -2.5081, -3.2260],\n",
              "        [-2.8485, -4.4494, -2.7608, -4.1834, -3.5099, -3.7831, -2.7568, -2.9310,\n",
              "         -3.0643, -3.1386, -4.0570, -3.2456, -3.6132, -4.0039, -4.0824, -2.6750,\n",
              "         -2.5780, -3.1183, -4.5324, -3.4260, -3.5911, -2.9311, -3.1366, -3.7858,\n",
              "         -3.1725, -3.6923, -3.5804],\n",
              "        [-2.9640, -3.3571, -3.7197, -2.1146, -3.6426, -2.8309, -3.4560, -2.5572,\n",
              "         -3.3709, -4.6924, -3.5777, -4.1055, -3.9993, -2.9213, -3.9707, -4.2070,\n",
              "         -4.2758, -4.2684, -4.0964, -2.5193, -3.3899, -3.0011, -3.6852, -3.2780,\n",
              "         -3.7208, -2.9188, -3.4737],\n",
              "        [-3.0256, -3.4814, -4.0457, -3.2602, -4.5282, -3.0842, -3.0334, -2.5029,\n",
              "         -3.6082, -3.7980, -3.5347, -3.3905, -3.2909, -3.7658, -4.4957, -3.9985,\n",
              "         -3.9962, -3.2768, -3.7659, -2.4077, -3.2397, -2.4330, -3.1670, -3.1202,\n",
              "         -3.1045, -4.1076, -3.4359],\n",
              "        [-3.5127, -4.2837, -3.5639, -3.9223, -3.4295, -3.1631, -3.4315, -3.4687,\n",
              "         -2.4982, -2.8158, -4.3324, -3.2578, -3.2829, -3.1644, -3.6720, -3.6159,\n",
              "         -2.8513, -3.0356, -3.1103, -3.7193, -2.6487, -3.6697, -2.6987, -3.9793,\n",
              "         -3.4437, -3.2499, -4.0921],\n",
              "        [-4.1646, -4.8590, -3.2915, -3.1108, -3.3962, -3.2632, -4.0177, -5.0365,\n",
              "         -3.3321, -3.0951, -3.6931, -3.9266, -2.9000, -3.5341, -3.1886, -1.9526,\n",
              "         -3.0129, -2.8711, -2.4426, -3.3431, -3.1102, -4.6294, -4.3876, -3.7537,\n",
              "         -3.2782, -3.6094, -3.7837],\n",
              "        [-3.7364, -3.4239, -3.7807, -1.6542, -4.4524, -2.8282, -2.7531, -3.0793,\n",
              "         -3.2161, -3.4809, -4.1962, -3.9449, -3.9668, -3.6289, -5.0155, -4.9627,\n",
              "         -3.8693, -3.3625, -4.0909, -2.9014, -2.9539, -3.1669, -3.9662, -3.2557,\n",
              "         -3.0343, -3.1554, -4.2044],\n",
              "        [-3.8356, -4.3195, -3.1127, -4.1926, -3.6271, -3.9780, -2.5079, -3.2779,\n",
              "         -2.9209, -3.0083, -4.0952, -3.1828, -3.2687, -3.5938, -4.1276, -3.2596,\n",
              "         -2.6020, -3.0499, -3.1059, -3.8141, -2.9996, -3.2610, -2.3111, -4.5509,\n",
              "         -3.7823, -3.6757, -3.6768],\n",
              "        [-3.7714, -3.5915, -2.9480, -3.8226, -4.0048, -2.5740, -3.5796, -4.1432,\n",
              "         -3.5788, -3.0973, -3.5491, -3.3704, -2.6974, -3.3544, -2.9825, -3.1349,\n",
              "         -3.5752, -3.4023, -2.4378, -4.2492, -2.7431, -4.1622, -4.0009, -3.3339,\n",
              "         -3.3527, -3.2598, -3.3819],\n",
              "        [-3.0105, -3.6899, -3.5497, -2.8847, -4.0252, -3.2271, -3.5349, -2.8847,\n",
              "         -3.0313, -3.0359, -3.4494, -3.0836, -3.0135, -3.3443, -3.6087, -3.7112,\n",
              "         -3.8408, -2.9099, -3.4719, -2.9574, -2.8470, -3.0683, -3.6268, -3.3624,\n",
              "         -3.8155, -3.8681, -3.8567],\n",
              "        [-3.1200, -2.8424, -3.4864, -3.9525, -3.8896, -2.4943, -3.5498, -3.7257,\n",
              "         -3.6781, -2.9479, -3.3686, -3.0226, -2.8374, -4.1547, -3.4860, -4.0617,\n",
              "         -3.2476, -3.8988, -2.7370, -4.0343, -3.1992, -3.6329, -3.4155, -2.7263,\n",
              "         -2.9089, -3.5795, -3.9434],\n",
              "        [-3.4977, -3.6850, -2.4542, -3.1137, -2.8009, -2.9368, -4.1216, -3.7217,\n",
              "         -3.9129, -3.9848, -3.0022, -3.8248, -3.1931, -2.9060, -2.6577, -2.7948,\n",
              "         -4.2023, -4.0303, -3.6936, -3.1119, -3.6067, -4.0194, -4.6219, -3.0282,\n",
              "         -3.7662, -3.0901, -3.0180],\n",
              "        [-2.8919, -2.4800, -3.0977, -2.8891, -3.2008, -3.3139, -3.8064, -2.6826,\n",
              "         -3.7458, -4.0253, -2.6654, -4.2054, -3.7770, -4.0229, -3.1343, -3.5557,\n",
              "         -4.1899, -3.4839, -3.9676, -2.9968, -3.7614, -3.0875, -3.6906, -3.8434,\n",
              "         -3.6550, -3.1677, -2.8908],\n",
              "        [-3.8356, -4.3195, -3.1127, -4.1926, -3.6271, -3.9780, -2.5079, -3.2779,\n",
              "         -2.9209, -3.0083, -4.0952, -3.1828, -3.2687, -3.5938, -4.1276, -3.2596,\n",
              "         -2.6020, -3.0499, -3.1059, -3.8141, -2.9996, -3.2610, -2.3111, -4.5509,\n",
              "         -3.7823, -3.6757, -3.6768],\n",
              "        [-3.7003, -3.8441, -3.3720, -4.0779, -3.2576, -3.2442, -2.8059, -3.0348,\n",
              "         -3.4482, -3.3330, -3.5909, -3.0498, -2.8229, -3.0012, -4.1722, -3.6810,\n",
              "         -3.5800, -3.8980, -3.3766, -2.4854, -2.8976, -3.3994, -3.1702, -2.9921,\n",
              "         -3.4178, -3.8648, -3.7172],\n",
              "        [-2.9304, -4.1996, -4.0313, -3.1931, -3.4965, -3.2272, -3.5394, -3.3023,\n",
              "         -2.9044, -3.8394, -2.7711, -3.3229, -2.8412, -3.8710, -3.1698, -2.6061,\n",
              "         -4.1093, -3.7816, -3.2292, -2.8919, -3.9310, -3.4450, -3.7306, -3.6337,\n",
              "         -3.2722, -2.8586, -3.3352],\n",
              "        [-2.8881, -2.6565, -2.9965, -3.0018, -3.6387, -2.8274, -3.4149, -3.1127,\n",
              "         -3.7236, -4.1145, -2.9612, -3.6012, -3.7707, -3.5097, -3.1818, -3.1423,\n",
              "         -3.9590, -3.2365, -3.8758, -2.4070, -4.3229, -3.4470, -4.1997, -3.5021,\n",
              "         -3.4223, -3.4948, -3.4401],\n",
              "        [-3.8356, -4.3195, -3.1127, -4.1926, -3.6271, -3.9780, -2.5079, -3.2779,\n",
              "         -2.9209, -3.0083, -4.0952, -3.1828, -3.2687, -3.5938, -4.1276, -3.2596,\n",
              "         -2.6020, -3.0499, -3.1059, -3.8141, -2.9996, -3.2610, -2.3111, -4.5509,\n",
              "         -3.7823, -3.6757, -3.6768],\n",
              "        [-3.0366, -4.0140, -3.6566, -4.3992, -2.8722, -3.5583, -2.8263, -3.4569,\n",
              "         -3.7615, -3.2210, -3.4927, -3.2022, -3.3919, -3.3752, -3.2738, -3.0640,\n",
              "         -2.6235, -3.7451, -3.2353, -4.1296, -3.9887, -3.7422, -2.1375, -4.0183,\n",
              "         -2.8535, -3.4990, -4.1546],\n",
              "        [-3.3750, -2.5529, -3.8226, -3.0395, -3.6164, -2.6998, -3.6908, -3.5558,\n",
              "         -3.6435, -3.5861, -2.3446, -4.0949, -3.4679, -4.4550, -3.2891, -3.9615,\n",
              "         -4.1100, -4.0952, -4.3317, -3.5854, -4.2628, -3.7134, -4.4582, -1.8395,\n",
              "         -2.9892, -3.6458, -3.0850],\n",
              "        [-2.7694, -3.6223, -4.2421, -3.8520, -3.4136, -2.3846, -3.2799, -3.4035,\n",
              "         -3.7264, -3.3770, -3.5128, -2.9726, -2.9512, -3.3894, -3.8986, -4.3342,\n",
              "         -3.4521, -3.8574, -2.8390, -4.2535, -3.6503, -3.8101, -2.6477, -2.9619,\n",
              "         -2.6350, -3.1614, -4.5870],\n",
              "        [-3.1480, -3.7608, -3.8847, -4.5860, -2.9990, -3.7088, -2.5536, -3.1943,\n",
              "         -3.6237, -2.9796, -4.0327, -2.9936, -2.8928, -3.5303, -3.5716, -3.9368,\n",
              "         -2.8869, -3.6078, -3.3916, -3.4124, -3.8028, -3.3568, -2.0408, -3.9749,\n",
              "         -3.1787, -3.6588, -4.6559],\n",
              "        [-3.5365, -3.5940, -3.0521, -3.8694, -3.3002, -3.6692, -3.3020, -3.5318,\n",
              "         -3.3418, -2.9893, -3.8874, -3.4901, -3.3031, -3.3429, -3.0832, -2.8016,\n",
              "         -3.1086, -3.0289, -2.9536, -4.1188, -2.3044, -4.1957, -2.9961, -4.4371,\n",
              "         -4.0741, -3.3584, -3.1801],\n",
              "        [-2.7177, -3.2664, -4.1979, -3.8374, -3.3341, -2.6990, -3.2383, -3.2154,\n",
              "         -4.6426, -3.6584, -3.1375, -3.5406, -3.7356, -3.8955, -3.5929, -3.5139,\n",
              "         -2.6568, -3.6807, -2.6670, -3.6796, -3.3250, -3.4214, -3.0740, -2.8794,\n",
              "         -2.5721, -4.4035, -4.0202],\n",
              "        [-3.7539, -4.0468, -3.4567, -3.4159, -2.9691, -3.7052, -2.7688, -3.4579,\n",
              "         -2.6689, -3.4527, -3.5715, -3.2157, -3.5068, -3.3253, -3.4536, -2.7346,\n",
              "         -3.5214, -3.5351, -3.2328, -3.5862, -3.1422, -3.5531, -2.8207, -4.2405,\n",
              "         -3.3809, -2.9070, -3.4264],\n",
              "        [-3.2730, -2.9346, -3.8248, -2.7253, -3.0863, -2.9592, -3.3298, -3.5293,\n",
              "         -3.6550, -3.8881, -3.3604, -3.7545, -4.1107, -3.7107, -4.5003, -3.4435,\n",
              "         -3.3712, -3.4836, -3.0824, -3.0309, -2.8522, -3.5773, -3.8536, -2.5915,\n",
              "         -2.4670, -3.8646, -3.7385],\n",
              "        [-3.0334, -3.7420, -2.7979, -3.9696, -2.5908, -3.5060, -4.1205, -3.7501,\n",
              "         -4.2005, -2.9357, -3.5925, -3.4352, -3.4297, -2.5275, -3.1212, -3.3223,\n",
              "         -2.9873, -3.1388, -3.1016, -4.3204, -2.8848, -4.4973, -2.7204, -3.8047,\n",
              "         -3.9716, -3.2339, -4.0214]], grad_fn=<LogBackward0>)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probs.log()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### counts_sum_inv = counts_sum**-1 \n",
        "\n",
        "if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "Okay, for dcountssuminv, (this bit is for normalisation, tldr yeah funny funs just so we can go through it step by step).\n",
        "Local derivative (ie w.r.t output) * output's gradient (w.r.t loss). But wait! there's a broadcast operation happening too\n",
        "And it's 1/..., so really our local deriviative would just be -1/...^2 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32, 1]))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "counts.shape, counts_sum_inv.shape\n",
        "# c = a * b (element wise!!) but with tensors, a[3,3], b[3,1] ------>\n",
        "# a11*b1 a12*b1 a13*b1 (Because we're broadcasting for element wise multiplication)\n",
        "# a21*b2 a22*b2 ...\n",
        "# ...\n",
        "# -> c[3,3]\n",
        "# So really, when we do probs = counts  counts_sum_inv, this is what's happening.\n",
        "# So in that  case, wouldn't the dcount_sum_invs be the a element?\n",
        "# Yes! since c = a*b, and well, b is our countsum inv, dc/db = a (our local derivative!)\n",
        "# So is dcounts_sum_inv = counts * dprobs?? Well, yes, but that's for our broadcasted b, not our single b column.\n",
        "# how we do gradients that connect to multiple nodes, is we have to sum them. I.e, we'd just add the gradients of all our b1s that were used, etc..\n",
        "# So dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # Horizontal sum, i.e sum 'in the direction of columns'\n",
        " "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dcounts\n",
        "Dcounts, like from the above example, is our 'a' basically. So it's gradient is just the b column, i.e our counts_sum_inv\n",
        "```dcounts = counts_sum_inv * dprobs```\n",
        "\n",
        "However, this is only the first contribution of dcounts, as we use the counts multiple times. We have to backprop into counts_sum, as \n",
        "count_sum_inv backprops into that (counts sum) which backrpobs into counts\n",
        "\n",
        "And dcounts_sum local derivative is:\n",
        "dcountssum_inv / dcounts_sum, and ince count_sum_inv = 1/counts_sum, then\n",
        "dcountsum_in/dcounts_sum = -1/(counts_sum**2). This is local derivative, so we just * by output der to get \n",
        "dL/dcounts_sum = local derivative * output :)\n",
        "\n",
        "A note:\n",
        "Say we have L = 3xy\n",
        "dL/dx = 3y. So in our notation, that'd be dx = 3y, yeah?\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### dnorm_logits\n",
        " dL/dnorm_logits:\n",
        "\n",
        "Well, the operation is just e^x, and very epicly, our derivative is also just... it self lmao..\n",
        "so local derivative dcounts/dnorm_logits = counts (i.e norm_logits.exp()).\n",
        "Then we just * dcounts to get the final dL/dnorm_logits"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### dlogit_maxes\n",
        "\n",
        "Well, okay, this one... hmm\n",
        "so norm_logits = logits - logit_maxes..\n",
        "dnorm/dlogitmaxes = -1. cool. \n",
        "\n",
        "But wait, **it's not that simple!!!** The shapes!!!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm_logits.shape, logits.shape, logit_maxes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# c11 c12 c13 = a11 a12 a13     b1\n",
        "# c21 c22 c23 = a21 a22 a23  -  b2\n",
        "# c31 c32 c33 = a31 a32 a33     b3\n",
        "\n",
        "# e.g so c32 = a32 - b3\n",
        "# Cool, so our dlogits would just be 1 * dnorm_logits here,\n",
        "# and db or dlogit_maxes = -1 * dnorm_logits, but we'd have to do the summing as we've done above to fix up the broadcasting, as basically the 'b1' would occur on every element of c, so you'd have to sum up all the a1s to get the b1 gradient :D \n",
        "\"\"\"\n",
        "# e.g: c1 = a1b1 + a2b1 + a3b1\n",
        "# dc1/db1 = a1 + a2 + a3 OH SHIT IT MAKES PERFECT SENSE. SO YOU SUM UP THE GRADS! \n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "and then we'd just * our output gradient once again, chain ruling it up to get dL/dlogit_maxes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### back to logits\n",
        "Okay, we're at our logt_maxes = logits.max(1...).\n",
        "\n",
        "Basically, the max thingy is plucking out the values from logits where it's the max. It also gives us the indices of where they came from, which is very useful for backpropagation.\n",
        "We basically want to let the gradients 'pass through' the correct indices of logits.\n",
        "Karpathy did this with a one liner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 1,  2, 19,  4, 15, 25, 16,  3, 19,  8, 15,  3, 22,  5,  7,  5,  2, 10,\n",
              "        22, 19, 10, 19, 22, 22, 23,  5, 22, 20, 24,  6, 24, 13])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits.max(1).indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 27])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1])) # logits shape 1 is 27. This basically just makes a 32, 27 array, with each row having 0s and a 1 in the position of the correct index\n",
        "print(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]).shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logits and h, W2, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]),\n",
              " torch.Size([32, 64]),\n",
              " torch.Size([64, 27]),\n",
              " torch.Size([27]))"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logits.shape, h.shape, W2.shape, b2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.3317e-03, -4.8336e-03,  2.1100e-03,  ..., -2.0401e-03,\n",
              "         -2.0078e-03, -1.9343e-03],\n",
              "        [-2.8817e-03,  2.2771e-03, -1.7434e-03,  ...,  1.0092e-03,\n",
              "         -3.1052e-03, -4.9481e-03],\n",
              "        [-6.6450e-05,  5.3056e-03,  2.6279e-03,  ...,  7.2947e-03,\n",
              "         -9.3339e-04,  1.0139e-03],\n",
              "        ...,\n",
              "        [-1.6354e-03,  2.5897e-03, -2.6171e-03,  ..., -4.3539e-03,\n",
              "         -2.6826e-03, -3.5181e-03],\n",
              "        [-3.5565e-03,  8.5893e-04, -5.0359e-03,  ...,  5.2263e-04,\n",
              "          2.8454e-03,  5.1892e-03],\n",
              "        [ 3.5356e-04,  5.3225e-03,  3.2453e-03,  ..., -2.0260e-03,\n",
              "          3.2633e-03,  8.0382e-04]], grad_fn=<MmBackward0>)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dlogits @ W2.T"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### hpreact\n",
        "\n",
        "Right. So dL/dpreact. well, what'\n",
        "h = tanh(x).\n",
        "dh/dx ... Well. Local derivative of h (i.e dh/dhpreact) would just be derivative of tanh, i.e 1-tanh(x)**2. Lol\n",
        "So it's just 1-itself**2 for every element in the thing. Cool! (This is the local derivative), i.e:\n",
        "dh/dhpreact = 1 - h**2  !!! Yeah!!! Nice!\n",
        "\n",
        "And then ofc dL/dhpreact = dh/dhpreact * dL/dh = (1-h**2) * dh "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### bngain stuff etc\n",
        "We can just use our cheats etc by looking at the shapes and how we know we derive matrix stuff from the video/paper hehhehhee.... || OH SHIT NEVERMIND -- it's a fucking elementwise multiply lmaooo ya gronk\n",
        "\n",
        "So looks like (with help of below):\n",
        "Local derivatives\n",
        "dhpreact/dbngain = bnraw.sum(0) # because the element wise * broadcasted our bngain downwards (by row), so we're \"colleccting\" them again :) || Since element wise, we gotta (bnraw * dhpreact).sum(0) for it to pass through properly.\n",
        "\n",
        "dhpreact/bnraw = (bngain * hpreact) # The bngain broadcasts back to what we want so it's chill\n",
        "\n",
        "dhpreact/dbnbias = dhpreact.sum(0, keepdim=True) # sum the rows dimension, since our bnbias is being broadcasted that way (and thus we must 'collect' all the gradients that we attributed our element nodes to) || keepdim true because [1,64] for some reason.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 64]), torch.Size([1, 64]), torch.Size([32, 64]), torch.Size([1, 64])\n"
          ]
        }
      ],
      "source": [
        "print(f'{dhpreact.shape}, {bngain.shape}, {bnraw.shape}, {bnbias.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]), torch.Size([64, 1]))"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hpreact.shape, bngain.T.shape # Gotta multiply this way to end well"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### bndiff, bnvar_inv\n",
        "So... bitwise multiplication again,let's start with bnvar_inv\n",
        "dbnraw/bnvar_inv = bndiff, but we do have to be careful with shapes and broadcasting\n",
        "\n",
        "dbnraw/bnvar_inv = bndiff.sum(0, keepdim=True).shape #technically true\n",
        "And for bndiff, it'll just be bnvar_inv but in the shape of bndiff (i.e copy bnvar_inv downwards)\n",
        "So dL/dbnvar_inv = dbvar_inv = ^^ above * dbnraw  ## WELL, LOOK BELOW. YOU HAVE TO DO THE ELEMENT WISE * FIRST, AND THEN YOU CAN SUM THEM.\n",
        "\n",
        "Wait so:\n",
        "```\n",
        "dbndiff = bnvar_inv * dbraw # this works since our broadcasting is fine\n",
        "dbnvar_inv = (bndiff * dbraw).sum(0, keepdim=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dbnraw.shape, bndiff.shape, bnvar_inv.shape\n",
        "# bnvar_inv is broadcasted all the way down (duplicate all the row vectors) to bndiff, so it's basically distributing all its elements over everything in bndiff.\n",
        "# I.e, to get the derivative of bnvar_inv, it's not just bndiff, we also have to sum it up row wise (dim 0) to \"collect\" all our elements' gradients back. And we keepdims because it's bnvar_inv is [1,64] for some reason"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### dbnvar time, and then we dbndiff2\n",
        "well, dl/dbnvar time. Local deriviative so dbnvar_inv / dbnvar: so the function is 1/sqrt(x+1e5)..\n",
        "-0.5 *(x+1e5)**-1.5\n",
        "\n",
        "Cool! NNow let's try dbnvar / dbndiff 2 which is the summy thingy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbnvar = -0.5 *( bnvar+1e-5)**-1.5 # local derivative\n",
        "dbnvar *= bnvar_inv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 64])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bnvar.shape\n",
        "bndiff2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 64])\n",
            "torch.Size([1, 64])\n",
            "torch.Size([32, 64])\n",
            "torch.Size([32, 64])\n"
          ]
        }
      ],
      "source": [
        "print((bndiff2.sum(0, keepdim=True) / (n-1)).shape) #well, would the gradient just be 1/n-1? because like, say y=x. dy/dx = 1. duh. dbnvar/dbdiff2 = 1 for all elements if there wasn't the 1/(n-1) scaling. Pog!\n",
        "print(bnvar.shape)\n",
        "## lmao, it would!!!\n",
        "print((torch.ones_like(bndiff2) * 1/(n-1)).shape) # Local derivaitve dbnvar/dbndiff2is just all 1/(n-1) because we're distributing that gradient over all the elements.\n",
        "## And we get the full derivative (chain ruled) byu just multiplying -- broadcasting will do it the way we want to.\n",
        "print(bndiff2.shape) # what we want our dnbiff2.shape to look like (since well, yeah... we want the shapes of the gradient tensor to be the same as the bloody numbers we're doing it on)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# While we're att it, dbndiff2 / dbndiff = 2bndiff. yay! :D \n",
        "# second part of dbndiff done. that's dnbdiff done! YOOOOO IT WORKS!!!!!!\n",
        "# so we dbndiff += 2bndiff * dbndiff2, as the first part was calculated before."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### From bndiff= hprebn - bnmeani\n",
        "\n",
        "Let's do dbndiff / dbnmeani:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bndiff.shape, hprebn.shape, bnmeani.shape\n",
        "## Hohoohho so bnmeani gets broadcasted in the forward pass.. hmmm.... I wonder if \n",
        "# OH MY GOOOOD!!! we have to sum it in the backwards pass wahaaaaat to 'collect' our distributed gradients that are attached to the elements of bnmeani"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 64])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dhprebn = 1* dbndiff.clone() # same shape, so we just 'pass the gradients' like that. || This is our dL/dhprebn. dbndiff/dhprebn = 1. Ofc this isn't the full dL/dhprebn\n",
        "dbnmeani = (-torch.ones_like(bndiff) * dbndiff).sum(0)# local gradient of -1 since it's just ... - bnmeani, and then we sum our output gradients back in order to 'collect' the broadcasted elements back again. Can be simplified to -dbndiff.sum(0)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hprebn etc... \n",
        "\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation. Basically just the same as layer 2 stuff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "layer 2 shapes from before:  torch.Size([32, 27]) torch.Size([32, 64]) torch.Size([64, 27]) torch.Size([27])\n",
            "layer 1:  torch.Size([32, 64]) torch.Size([32, 64]) torch.Size([32, 30]) torch.Size([30, 64]) torch.Size([64])\n",
            "torch.Size([32, 64])\n"
          ]
        }
      ],
      "source": [
        "print('layer 2 shapes from before: ', dlogits.shape, h.shape, W2.shape, b2.shape)\n",
        "print('layer 1: ', hprebn.shape, dhprebn.shape, embcat.shape, W1.shape, b1.shape)\n",
        "# Yep, just same stuff as before.\n",
        "# Make sure the shapes fit well soo like it'd be\n",
        "# dembcat = dphrebn * W1.T -> (32, 64) * (64, 30)\n",
        "print(dh.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 30]) torch.Size([32, 3, 10])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 10])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##### Forward pass: embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "print(embcat.shape, emb.shape)\n",
        "# Really, all we did was flatten the 3rd dimension into the 2nd one, yknow?\n",
        "# View just interprets the way the array is arranged lol so we can put it back to its normal shape.\n",
        "demb = dembcat.view(emb.shape) # can pass tuples into views :o \n",
        "demb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 3, 10]) torch.Size([27, 10]) torch.Size([32, 3])\n",
            "tensor([[ 1,  1,  4],\n",
            "        [18, 14,  1],\n",
            "        [11,  5,  9],\n",
            "        [ 0,  0,  1],\n",
            "        [12, 15, 14]])\n"
          ]
        }
      ],
      "source": [
        "# emb = C[Xb]\n",
        "print(emb.shape, C.shape, Xb.shape)\n",
        "# So basically, we're only selecting whatever C elements are in Xb\n",
        "# Xb is 32 examples of indices like [1,1,4], [2,3,1] .... And then, these indices are looked up in our C embedding, which is where we embedded all our vocab (alphabet) into some 10 dimensional vector.\n",
        "print(Xb[:5])\n",
        "## And remember, now we have demb. We have their gradients.\n",
        "# So we just have to route back those gradients into whichever row/element of C they came from!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 4.3405e-04, -1.5912e-03, -5.3613e-06,  2.1225e-03, -1.7826e-03,\n",
              "        -2.8640e-03,  4.1290e-03,  5.9210e-04, -6.2336e-04,  1.1974e-03],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demb[0, 0] # This is the encoding (embedding) for a '., .'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1) 0 0 torch.Size([10])\n",
            "tensor(1) 0 1 torch.Size([10])\n",
            "tensor(4) 0 2 torch.Size([10])\n",
            "tensor(18) 1 0 torch.Size([10])\n",
            "tensor(14) 1 1 torch.Size([10])\n",
            "tensor(1) 1 2 torch.Size([10])\n",
            "tensor(11) 2 0 torch.Size([10])\n",
            "tensor(5) 2 1 torch.Size([10])\n",
            "tensor(9) 2 2 torch.Size([10])\n",
            "tensor(0) 3 0 torch.Size([10])\n",
            "tensor(0) 3 1 torch.Size([10])\n",
            "tensor(1) 3 2 torch.Size([10])\n",
            "tensor(12) 4 0 torch.Size([10])\n",
            "tensor(15) 4 1 torch.Size([10])\n",
            "tensor(14) 4 2 torch.Size([10])\n",
            "tensor(0) 5 0 torch.Size([10])\n",
            "tensor(17) 5 1 torch.Size([10])\n",
            "tensor(1) 5 2 torch.Size([10])\n",
            "tensor(0) 6 0 torch.Size([10])\n",
            "tensor(0) 6 1 torch.Size([10])\n",
            "tensor(13) 6 2 torch.Size([10])\n",
            "tensor(13) 7 0 torch.Size([10])\n",
            "tensor(13) 7 1 torch.Size([10])\n",
            "tensor(1) 7 2 torch.Size([10])\n",
            "tensor(8) 8 0 torch.Size([10])\n",
            "tensor(25) 8 1 torch.Size([10])\n",
            "tensor(12) 8 2 torch.Size([10])\n",
            "tensor(0) 9 0 torch.Size([10])\n",
            "tensor(0) 9 1 torch.Size([10])\n",
            "tensor(26) 9 2 torch.Size([10])\n",
            "tensor(22) 10 0 torch.Size([10])\n",
            "tensor(15) 10 1 torch.Size([10])\n",
            "tensor(14) 10 2 torch.Size([10])\n",
            "tensor(19) 11 0 torch.Size([10])\n",
            "tensor(13) 11 1 torch.Size([10])\n",
            "tensor(9) 11 2 torch.Size([10])\n",
            "tensor(0) 12 0 torch.Size([10])\n",
            "tensor(0) 12 1 torch.Size([10])\n",
            "tensor(0) 12 2 torch.Size([10])\n",
            "tensor(0) 13 0 torch.Size([10])\n",
            "tensor(4) 13 1 torch.Size([10])\n",
            "tensor(5) 13 2 torch.Size([10])\n",
            "tensor(5) 14 0 torch.Size([10])\n",
            "tensor(14) 14 1 torch.Size([10])\n",
            "tensor(9) 14 2 torch.Size([10])\n",
            "tensor(18) 15 0 torch.Size([10])\n",
            "tensor(5) 15 1 torch.Size([10])\n",
            "tensor(5) 15 2 torch.Size([10])\n",
            "tensor(0) 16 0 torch.Size([10])\n",
            "tensor(4) 16 1 torch.Size([10])\n",
            "tensor(1) 16 2 torch.Size([10])\n",
            "tensor(1) 17 0 torch.Size([10])\n",
            "tensor(18) 17 1 torch.Size([10])\n",
            "tensor(1) 17 2 torch.Size([10])\n",
            "tensor(0) 18 0 torch.Size([10])\n",
            "tensor(0) 18 1 torch.Size([10])\n",
            "tensor(0) 18 2 torch.Size([10])\n",
            "tensor(0) 19 0 torch.Size([10])\n",
            "tensor(5) 19 1 torch.Size([10])\n",
            "tensor(12) 19 2 torch.Size([10])\n",
            "tensor(0) 20 0 torch.Size([10])\n",
            "tensor(10) 20 1 torch.Size([10])\n",
            "tensor(1) 20 2 torch.Size([10])\n",
            "tensor(9) 21 0 torch.Size([10])\n",
            "tensor(14) 21 1 torch.Size([10])\n",
            "tensor(1) 21 2 torch.Size([10])\n",
            "tensor(0) 22 0 torch.Size([10])\n",
            "tensor(0) 22 1 torch.Size([10])\n",
            "tensor(0) 22 2 torch.Size([10])\n",
            "tensor(0) 23 0 torch.Size([10])\n",
            "tensor(0) 23 1 torch.Size([10])\n",
            "tensor(18) 23 2 torch.Size([10])\n",
            "tensor(20) 24 0 torch.Size([10])\n",
            "tensor(5) 24 1 torch.Size([10])\n",
            "tensor(1) 24 2 torch.Size([10])\n",
            "tensor(0) 25 0 torch.Size([10])\n",
            "tensor(11) 25 1 torch.Size([10])\n",
            "tensor(15) 25 2 torch.Size([10])\n",
            "tensor(0) 26 0 torch.Size([10])\n",
            "tensor(0) 26 1 torch.Size([10])\n",
            "tensor(7) 26 2 torch.Size([10])\n",
            "tensor(0) 27 0 torch.Size([10])\n",
            "tensor(18) 27 1 torch.Size([10])\n",
            "tensor(5) 27 2 torch.Size([10])\n",
            "tensor(26) 28 0 torch.Size([10])\n",
            "tensor(5) 28 1 torch.Size([10])\n",
            "tensor(18) 28 2 torch.Size([10])\n",
            "tensor(0) 29 0 torch.Size([10])\n",
            "tensor(0) 29 1 torch.Size([10])\n",
            "tensor(14) 29 2 torch.Size([10])\n",
            "tensor(3) 30 0 torch.Size([10])\n",
            "tensor(5) 30 1 torch.Size([10])\n",
            "tensor(14) 30 2 torch.Size([10])\n",
            "tensor(0) 31 0 torch.Size([10])\n",
            "tensor(18) 31 1 torch.Size([10])\n",
            "tensor(15) 31 2 torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "dC = torch.zeros_like(C)\n",
        "# For every element in the Xb (which is like the indices stuff -- [1,1,4], ...)\n",
        "for k in range(Xb.shape[0]):\n",
        "\tfor j in range(Xb.shape[1]):\n",
        "\t\tix = Xb[k,j] # the value; the alphabet position index (e.g 1 -> a)\n",
        "\t\tprint(ix, k,j, demb[k,j].shape) ## Prints size 10. I.e, this is the vector embedding. Well, this is the GRADIENT of the vector embedding!!!\n",
        "\t\tdC[ix] += demb[k, j] # We add the GRADIENT of the embedding vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mO-8aqxK8PPw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: backprop through the whole thing manually, || lots of explaining code above for our things\n",
        "# backpropagating through exactly all of the variables \n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "# -----------------\n",
        "## Ahh I got why I kept getting a bit 'wait is this right..' because I'm typing dlogprobs etc.. instead of dL/dlogprobs first, so I'm like uhhhh\n",
        "# Say we have L = 3xy\n",
        "# dL/dx = 3y. So in our notation, that'd be dx = 3y, yeah?\n",
        "# There we go. It should be good now. Basically, when we're trying to find 'dx', that's really dL/dx = ... so it's good.\n",
        "\n",
        "### I.e when we write dh, really we mean dL/dh, or dldh, but we don't write the dl because words... but still it's so aaaaaahhh chungy\n",
        "# Alll ouur derivatives w.r.t loss\n",
        "dlogprobs = torch.zeros_like(logprobs) # makes zeros matrix same shape as logprobs\n",
        "dlogprobs[range(n), Yb] = -1.0/n # derivative of Loss w.r.t logprobs\n",
        "\n",
        "dprobs = (1.0/probs) * dlogprobs # Local derivative * outer gradient. Intuitively, it's making any examples wiht low probabilities assigned, and boosting t heir gradient || this be dLoss/dprobs \n",
        "\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # Horizontal sum of local derivative (counts) and output derivative/gradient (dprobs), sum along because that's how you add gradients for multiple nodes || Really good explanation written for this working in my dlogit_maxes working out above!!!!\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "dcounts += (torch.ones_like(counts)* dcounts_sum) # this part is the counts_sum section, etc... Basically the summing acts like a router of the inflowing gradient\n",
        "\n",
        "dnorm_logits = counts *dcounts # Local derivative \n",
        "dlogits = 1 * dnorm_logits.clone() # for safety :o. Also, this isn't the final dlogits!\n",
        "dlogit_maxes = - 1 * dnorm_logits.sum(1, keepdim=True) # Funny thing, this is basically all 0s right, because all this is doing is stabilising our logits so that they're all <=0, so we can .exp() without overflowing anything. And you can see, that all the gradients here are basically 0, reflecting the fact that changing these logit values doesnt change the loss (and also the probs).\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes # lgoit maxes is (32,1), and look at the logits section. This basically routes the correct logit max to wherever it needs to go\n",
        "\n",
        "# Now onto our layers -- did some math on paper -- see video\n",
        "dh = dlogits @ W2.T # dL/dh = dL/dlogits * W2 transpose || (32, 27) * (27, 64) -> (32, 64) == h.shape\n",
        "dW2 =  h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1 - h **2) * dh\n",
        "\n",
        "# ## Bnbias part\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True) # Since all this is element wise *, yo! || Keeping keepdim on because we initialised as [1,64] and not just [64] for whatever reason (he said idk why lmao)\n",
        "dbnraw = bngain * dhpreact \n",
        "dbnbias = dhpreact.sum(0, keepdim=True) # keepdim bc [1,64] for some reason again\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "\n",
        "dbndiff = bnvar_inv * dbnraw # broadcasting fixes it for us. But this isn't it; bndiff is used many times || remember, dL/dbndiff\n",
        "dbnvar = (-0.5 *( bnvar+1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = (torch.ones_like(bndiff2) * 1/(n-1)) * dbnvar\n",
        "\n",
        "dbndiff += 2*bndiff * dbndiff2 # second part of (dL/)bndiff\n",
        "\"\"\"Ahaha, notice how:\n",
        "when we have a sum in the forward pass, it ends up being a replication/broadcasting thing in the backwards (to 'distribute' our gradients').\n",
        "Conversely, when we have a replication/broadcast in the forward pass, we end up summing in the backwards pass (to 'collect' our gradients). Pog. \"\"\"\n",
        "dhprebn = 1 * dbndiff.clone() # same shape, so we just 'pass the gradients' like that. || This is our dL/dhprebn. dbndiff/dhprebn = 1\n",
        "dbnmeani = (-dbndiff).sum(0) # local gradient of -1 since it's just ... - bnmeani, and then we sum our output gradients back in order to 'collect' the broadcasted elements back again.\n",
        "dhprebn +=  1.0/n *( torch.ones_like(hprebn) * dbnmeani) # dl/dhprebn\n",
        "\n",
        "## layer 1 shit\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn # || Need to end up with (30, 64) W1.shape. So do that by embcat.T (30, 32) @ (32, 64) dhprebn. ez. \n",
        "db1 = 1 * dhprebn.sum(0) # || Remember, this is broadcasted aswell. So local derivative is just 1 since it's addition. But, we have to sum the rows because that's how the broadcasted distributed the b1 vector, so now we 'collect' those nodes again.\n",
        "\n",
        "demb = dembcat.view(emb.shape)\n",
        "dC = torch.zeros_like(C)\n",
        "# For every element in the Xb (which is like the indices stuff -- [1,1,4], ...)\n",
        "for k in range(Xb.shape[0]):\n",
        "\tfor j in range(Xb.shape[1]):\n",
        "\t\tix = Xb[k,j] # the value; the alphabet position index (e.g 1 -> a)\n",
        "\t\t# print(ix, k,j, demb[k,j].shape) \n",
        "\t\tdC[ix] += demb[k, j] \n",
        "\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebLtYji_8PPw"
      },
      "outputs": [],
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(-2.8247, grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logprobs[1, 1] # For example 1, collect the first character.\n",
        "# For our deriving, probs[y] is just a single value; the probability of choosing the correct character index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32]) torch.Size([32, 27])\n",
            "tensor(32., grad_fn=<SumBackward0>)\n",
            "tensor(1., grad_fn=<SumBackward0>)\n",
            "logits!! torch.Size([32, 27]) tensor([ 8.1234e-01,  9.7460e-01, -4.9720e-01,  4.2870e-01, -3.9033e-01,\n",
            "         9.8613e-01, -3.8532e-01,  7.1250e-02, -6.9734e-01,  5.1466e-04,\n",
            "         2.9390e-01,  7.6986e-02,  1.6035e-01, -1.5084e-01,  1.2482e-01,\n",
            "        -7.9750e-01, -1.3046e+00, -4.6557e-01, -7.3601e-01,  4.7255e-01,\n",
            "         5.0036e-01, -2.7683e-01, -2.7102e-01,  8.0077e-01,  6.4866e-01,\n",
            "        -7.3495e-02, -4.2614e-01], grad_fn=<SelectBackward0>) tensor(-0.1203, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(Yb.shape, probs.shape) # 32 examples, each example 27 probs (referring to probability of that char being chosen)\n",
        "print(probs[Yb].sum()) # because 32 examples\n",
        "print(probs[0].sum())\n",
        "\n",
        "## Basically, dlogits: dloss/dlogits = P_i if i !=y, and P_i - 1 if i=y\n",
        "# other places have it like p_i - y_i (basically, y would be a vector of all 0s except for one 1). This would be for an individual example. P_i is the probability distribution of that i'th element.\n",
        "\n",
        "## What it's saying is that, the logits \n",
        "print(\"logits!!\", logits.shape, logits[0], logits[0].sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-gCXbB4C8PPx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dlogits = F.softmax(logits, 1) # softmax converts logits -> prob distrs. \n",
        "# Softmax row wise ofc, because the row is our 27 chars possibilities.\n",
        "# ^ Above sets all dloss/dlogits to P_i.\n",
        "dlogits[range(n), Yb] -= 1 # Do the P_i - 1 bit for the correct character, for every training example in the batch (n=32 here)\n",
        "dlogits /= n # Loss is average loss, so we need to scale down gradients by the mean thingy.\n",
        "\n",
        "# -----------------\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x2aadec761f0>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')\n",
        "# Black squares are the positions of the correct indices, where we\n",
        "# did the -1 on dlogits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0714, 0.0839, 0.0193, 0.0486, 0.0214, 0.0849, 0.0215, 0.0340, 0.0158,\n",
              "        0.0317, 0.0425, 0.0342, 0.0372, 0.0272, 0.0359, 0.0143, 0.0086, 0.0199,\n",
              "        0.0152, 0.0508, 0.0522, 0.0240, 0.0242, 0.0705, 0.0606, 0.0294, 0.0207],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's look at the first row for probability\n",
        "F.softmax(logits, 1)[0]  # prob distr of row 1 for logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.0714,  0.0839,  0.0193,  0.0486,  0.0214,  0.0849,  0.0215,  0.0340,\n",
            "        -0.9842,  0.0317,  0.0425,  0.0342,  0.0372,  0.0272,  0.0359,  0.0143,\n",
            "         0.0086,  0.0199,  0.0152,  0.0508,  0.0522,  0.0240,  0.0242,  0.0705,\n",
            "         0.0606,  0.0294,  0.0207], grad_fn=<MulBackward0>)\n",
            "tensor(1.6298e-09, grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(dlogits[0] * n)# Bringing it back up from the mean to its original values || See how we got dlogits lmao || (i.e just the probabilities once again, but with our -1 here!)\n",
        "# ^^ Is just so it's easier to interpret for us.\n",
        "print(dlogits[0].sum()) # sums to 0 "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Do you see how everything is the same between the actual prob distribution and the dlogits, except for the -1 at the correct char index?\n",
        "\n",
        "And the sum of dlogits' rows like dlogits[0].sum() == 0. Woah!! It's like a force; you're pulling down on the probabiltities that aren't correct, and tugging up the probability that is correct. And it's like some sort of fabric.\n",
        "\n",
        "Like, the probs that are wrong, but with high values, will end up going *down* a lot more (since their P_i is also high and thus so is their gradient), whilst those with low probabilties won't go down too much. And the one with the right probability should go up more.\n",
        "\n",
        "And the amount of total push/pull is equalised since our sum is 0!! I.e, the amount that our good probability goes up == sum of all the bad probabilities going down! It 'takes' from the other ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hd-MkhB68PPy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]), torch.Size([32, 64]))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hprebn.shape, hpreact.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([6, 6])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.tensor([[1,2,3],[1,2,3]])\n",
        "a\n",
        "a.sum(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 64])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dhpreact.shape\n",
        "## OOooooh, I get why it's sum(0) and not column wise.\n",
        "# okay, so dl/dy == dhpreact, yep, and that's (32,64).\n",
        "\n",
        "# Now for the summation -sum(dl/dy_j for j=1 to m), we're summing to the number of training examples, you see? m = num training examples = n in the code here. But on our paper, yeah.\n",
        "# So we're actually summing by row here, so we want dhpreact.sum(0), as that's \n",
        "# == sum_j=1 to m( dl/dy_j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]), torch.Size([32, 64]))"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bnraw.shape, dhpreact.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "POdeZSKT8PPy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dhprebn = None # TODO. my solution is 1 (long) line\n",
        "# dl/dy is dhpreact.\n",
        "cons1 = bngain * bnvar_inv / n\n",
        "firstseg = -dhpreact.sum(0)\n",
        "secseg = -n/(n-1) * bnraw * (bnraw * dhpreact).sum(0)\n",
        "thirdseg = n * dhpreact\n",
        "\n",
        "dhprebn = cons1 *(firstseg + secseg + thirdseg) ## Nice. That's it.\n",
        "# -----------------\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "wPy8DhqB8PPz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12297\n",
            "      0/   3000: 3.8234\n"
          ]
        }
      ],
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 20000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO). Since, we aren't gonna call .backward() on anything... since we IMPLEMENTED IT BY HAND!!\n",
        "with torch.no_grad():\n",
        "\n",
        "    # kick off optimization\n",
        "    for i in range(max_steps):\n",
        "\n",
        "        # minibatch construct\n",
        "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "        Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "        # forward pass\n",
        "        emb = C[Xb] # embed the characters into vectors\n",
        "        embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "        # Linear layer\n",
        "        hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "        # BatchNorm layer\n",
        "        # -------------------------------------------------------------\n",
        "        bnmean = hprebn.mean(0, keepdim=True)\n",
        "        bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "        bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "        bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "        hpreact = bngain * bnraw + bnbias\n",
        "        # -------------------------------------------------------------\n",
        "        # Non-linearity\n",
        "        h = torch.tanh(hpreact) # hidden layer\n",
        "        logits = h @ W2 + b2 # output layer\n",
        "        loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "        # backward pass\n",
        "        for p in parameters:\n",
        "            p.grad = None\n",
        "        #   loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "        # manual backprop! #swole_doge_meme\n",
        "        # -----------------\n",
        "        # YOUR CODE HERE :)\n",
        "        # Get to dloss/dlogits\n",
        "        dlogits = F.softmax(logits, 1)\n",
        "        dlogits[range(n), Yb] -= 1 \n",
        "        dlogits /= n\n",
        "        # backprop through second layer\n",
        "        dh = dlogits @ W2.T \n",
        "        dW2 =  h.T @ dlogits\n",
        "        db2 = dlogits.sum(0)\n",
        "        dhpreact = (1 - h **2) * dh\n",
        "        \n",
        "        \n",
        "        ### Backproping through batchnorm:\n",
        "        dbngain = (bnraw * dhpreact).sum(0, keepdim=True) # Want to update\n",
        "        dbnbias = dhpreact.sum(0, keepdim=True) # Want to update\n",
        "        dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0)) # One lined it. || Speedrun to the preactivation (i.e skip all intermediary backpropping through batchnorm)\n",
        "\n",
        "        # Backpropping through layer 1\n",
        "        dembcat = dhprebn @ W1.T\n",
        "        dW1 = embcat.T @ dhprebn\n",
        "        db1 = 1 * dhprebn.sum(0)\n",
        "\n",
        "        # Backprop and get gradient for our C embedding matrix\n",
        "        demb = dembcat.view(emb.shape)\n",
        "        dC = torch.zeros_like(C)\n",
        "        for k in range(Xb.shape[0]):\n",
        "            for j in range(Xb.shape[1]):\n",
        "                ix = Xb[k,j]\n",
        "                dC[ix] += demb[k, j] \n",
        "\n",
        "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias] # These are all the gradients we actually care about; the things we will actually update. All the other shit is basically just intermediary stuff.\n",
        "        # -----------------\n",
        "\n",
        "        # update\n",
        "        lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "        for p, grad in zip(parameters, grads):\n",
        "            # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "            p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "        # track stats\n",
        "        if i % 5000 == 0: # print every once in a while\n",
        "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "        lossi.append(loss.log10().item())\n",
        "        # print(i)\n",
        "        #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "        #     break\n",
        "## output of this cell a bit different because I changed it a bit, but yeah. Should be fine to run it again: No loss.backward() required HOOOOLY custom implementation achieved. ez backprop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ZEpI0hMW8PPz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(27, 10)        | exact: False | approximate: True  | maxdiff: 1.862645149230957e-08\n",
            "(30, 200)       | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "(200,)          | exact: False | approximate: True  | maxdiff: 4.889443516731262e-09\n",
            "(200, 27)       | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n",
            "(27,)           | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "(1, 200)        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
          ]
        }
      ],
      "source": [
        "# useful for checking your gradients\n",
        "for p,g in zip(parameters, grads):\n",
        "  cmp(str(tuple(p.shape)), g, p)\n",
        "  # Perfect. Tiny ass differences, they're basically the same thing lol. We've done it right!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "KImLWNoh8PP0"
      },
      "outputs": [],
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "6aFnP_Zc8PP0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train 2.2129976749420166\n",
            "val 2.229011297225952\n"
          ]
        }
      ],
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "esWqmhyj8PP1"
      },
      "outputs": [],
      "source": [
        "# I achieved:\n",
        "# train 2.0718822479248047\n",
        "# val 2.1162495613098145\n",
        "# || He used 200k steps though @.@"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "xHeQNv3s8PP1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "carmah.\n",
            "ambelleigh.\n",
            "mri.\n",
            "reety.\n",
            "salansa.\n",
            "jazonto.\n",
            "amerynt.\n",
            "kaeli.\n",
            "ner.\n",
            "kentce.\n",
            "ihvon.\n",
            "leigh.\n",
            "ham.\n",
            "join.\n",
            "quinn.\n",
            "saline.\n",
            "kaidel.\n",
            "wantho.\n",
            "dearixi.\n",
            "jace.\n"
          ]
        }
      ],
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "    \n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "    \n",
        "    print(''.join(itos[i] for i in out))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "c8a31c3d3c58cfe49314f156da5c5377d35db16854b5a4a4800d645f76ea86e2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
