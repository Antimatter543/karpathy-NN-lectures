{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karpathy has CHALLENGED US TO BEAT HIS VALIDATION LOSS OF 2.2 (dev set loss).\n",
    "WE MUST!!! ONWARDS!!!! (We'll change the hyperparameters hbehehe)\n",
    "Also, he said we should be able to read a good chunk of the paper by now, so let's try that too :D\n",
    "\n",
    "Anyways, for this, let's make nice functions where we can change all the hyperparameters.\n",
    "Batch size, # neurons, C embedding size, lr, ... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read words and generate our char <-> int functions\n",
    "words = open('../2 - makemore/names.txt', 'r').read().splitlines() # Read everything\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # string to integer\n",
    "stoi['.'] = 0\n",
    "itos = {i: s for s, i in stoi.items()} # int to string\n",
    "\n",
    "words[:8] # woo\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making our dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(words, block_size, doprint=False):\n",
    "\t\"\"\"returns X, Y datasets based on words list\n",
    "\tblock_size: context length: how many characters do we take to predict the next one?\n",
    "\t\"\"\"\n",
    "\tX, Y = [], []\n",
    "\n",
    "\tfor w in words:\n",
    "\t\tif doprint:\n",
    "\t\t\tprint(w)\t\n",
    "\t\tcontext = [0] * block_size # this would make [0, 0, ...] based on block_size\n",
    "\n",
    "\t\tfor ch in w + '.': \n",
    "\t\t\tix = stoi[ch] # index of char\n",
    "\t\t\tX.append(context)\n",
    "\t\t\tY.append(ix)\n",
    "\t\t\t\n",
    "\t\t\tif doprint:\n",
    "\t\t\t\tprint(''.join(itos[i] for i in context), '------>', itos[ix])\n",
    "\t\t\tcontext = context[1:] + [ix] # [0,0,0] -> [0,0, ix] like a rolling effect\n",
    "\n",
    "\tX = torch.tensor(X)\n",
    "\tY = torch.tensor(Y)\n",
    "\tprint(X.shape, Y.shape)\n",
    "\treturn X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters!!! CHANGE STUFF HERE\n",
    "block_size = 3 # num of past chars to use to predict next char\n",
    "embedding_size = 10\n",
    "num_neurons = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 32033\n",
      "torch.Size([182400, 3]) torch.Size([182400])\n",
      "torch.Size([22962, 3]) torch.Size([22962])\n",
      "torch.Size([22784, 3]) torch.Size([22784])\n"
     ]
    }
   ],
   "source": [
    "# Generate our train, dev, and test loss. We'll be trying to get dev loss < 2.2\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "print(f'number of words: {len(words)}')\n",
    "\n",
    "Xtr, Ytr = generate_dataset(words[:n1], block_size)\n",
    "Xdev, Ydev = generate_dataset(words[n1:n2], block_size)\n",
    "Xte, Yte = generate_dataset(words[n2:], block_size)\n",
    "# Of course, the sizes are way more massive than # word examples because these are the examples from the words (char stuff), not the words themselves."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, initialisation of our model! I.e, we'll initialise the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in total: 11897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 10]), torch.Size([30, 200]))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # This will be a 'global' generator\n",
    "def generate_parameters(block_size, embedding_size, num_neurons):\n",
    "\t# We booosting our hidden layer size\n",
    "\tC = torch.randn(27, embedding_size, generator=g) # Generate embedding of characters, e.g (27,10)\n",
    "\n",
    "\tW1 = torch.randn((embedding_size * block_size, num_neurons), generator=g) # first weights takes input from \n",
    "\tb1 = torch.randn(num_neurons, generator=g)\n",
    "\tW2 = torch.randn((num_neurons, 27), generator=g)\n",
    "\tb2 = torch.randn(27, generator=g)\n",
    "\tparameters = [C, W1, b1, W2, b2] # for easy summing parameters\n",
    "\n",
    "\tfor p in parameters:  # Turn on requires grad for our parameter matrices\n",
    "\t\tp.requires_grad = True\n",
    "\t\t\n",
    "\tprint(\"Number of parameters in total:\", sum(p.nelement() for p in parameters)) # num of parameters in total\n",
    "\n",
    "\treturn parameters\n",
    "parameters = generate_parameters(block_size, embedding_size, num_neurons)\n",
    "C, W1, b1, W2, b2 = parameters\n",
    "C.shape, W1.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a consolidation bit, we'd be wanting to do this to push it through the first layer\n",
    "```\n",
    "C[Xtr] @ W1 + b1 # -> doesn't work, as C[Xtr].shape -> torch.Size([182484, 3, 10]) || Each example has 3 inputs, and each char input has an embedding size of 10.\n",
    "W1 -> torch.Size([30, 200]). \n",
    "```\n",
    "We want to flatten the embedded stuff so each example just reads as a 30 length vector (or w.e the size of the thingy is), i.e ([# examples, block*embedding sizes ]). We do this via: \n",
    "```\n",
    "C[Xtr].view((-1, block_size*embedding_size)).shape -> torch.Size([182484, 30])\n",
    "C[Xtr].view((-1, block_size*embedding_size)) @ W1 + b1 # works! -> torch.Size([182484, 200]). Yay!\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now we'll have to do gradient descent on the model. \n",
    "\n",
    "(this is called the training loop or whatever)\n",
    "If I remember well,\n",
    "\n",
    "0th step; we can minibatch! To make the mfer run faster, even at some loss of accuracy of the gradient to descend, it's still much better overall.\n",
    "\n",
    "#### First step, is forward passing.\n",
    " We get the training data (specifically, training data from minibatch). We embed it, and flatten that embedding (using the efficient .view(..)), and then 'run it through' the layers by doing our activation on it. Then we run it through the second layer (our final one), giving us the logits.\n",
    "\n",
    "We can get the loss straight up by doing F.cross_entropy, or we can get the probability dist. by softmaxing it. (And then you could work out loss. Remember, these functions are doing very simple things we've already done!! -- they just exp, normalise weights, and then get the negative loss likelihood).\n",
    "\n",
    "#### The backwards pass\n",
    "Make sure to set all our grads to 0.\n",
    "Then we just  loss.backward(), propagating through every operation done and getting it's gradient w.r.t loss\n",
    "\n",
    "#### Updating weights\n",
    "And we just update the weights now :D \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemented hardcode first\n",
    "\n",
    "def train(X: torch.Tensor, Y: torch.Tensor, num_steps: int, block_size: int, embedding_size: int, parameters): \n",
    "\t\"\"\"_summary_\n",
    "\n",
    "\tArgs:\n",
    "\t\tX (torch.Tensor): X dataset to train on (train with our Xtr!)\n",
    "\t\tY (torch.Tensor): Y dataset for labels\n",
    "\t\tnum_steps (int): Number of times to go through loop\n",
    "\t\tblock_size (int): Context length; # characters used to predict next one\n",
    "\t\tembedding_size (int): Size of embedding for the characters\n",
    "\t\tparameters (_type_): param\n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\tC, W1, b1, W2, b2 = parameters\n",
    "\n",
    "\tfor i in range(num_steps):\n",
    "\t\tix = torch.randint(0, X.shape[0], (32,)) # We get {minibatch_size} random ints from size of 0- training set\n",
    "\t\t\n",
    "\t\t# forward pass -- on our minibatch :o !!!!\n",
    "\t\temb = C[X[ix]] # (32,3,2) here || (minibatch_size, block_size, embed size))\n",
    "\t\t# print(f'{emb.shape=}')\n",
    "\t\th = torch.tanh(emb.view(-1, block_size*embedding_size) @ W1 + b1) # We do the whole embed flatten, push it through layer 1, and tanh activation on it. All elements of h is between [-1, 1]\n",
    "\t\tlogits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "\n",
    "\t\tloss = F.cross_entropy(logits, Y[ix]) # Does the epic normalisation stuff for us \n",
    "\t\t# print(f'Loss during this minibatch loop: {loss=}')\n",
    "\n",
    "\t\t# backward pass - zero grad, backprop\n",
    "\t\tfor p in parameters:\n",
    "\t\t\tp.grad = None\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\t# update\n",
    "\t\t# lr = lrs[i]\n",
    "\t\tlr = 0.1 if i < 20000 else 0.01\n",
    "\t\tfor p in parameters:\n",
    "\t\t\tp.data += -lr * p.grad\n",
    "\n",
    "\t\t# track stats (optional part)\n",
    "\n",
    "\treturn loss # Not needed, I'm just putting it here because why not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2736, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_loss = train(Xtr, Ytr, 30000, block_size, embedding_size, parameters) # \n",
    "print(train_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! Now let's evaluate our model!\n",
    "(Our model is our parameters :ooooo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(X, Y, block_size, embedding_size, parameters):\n",
    "\tC, W1, b1, W2, b2 = parameters\n",
    "\temb = C[X] # (X.shape[0],3,2) here -- whole set in this case\n",
    "\th = torch.tanh(emb.view(-1, block_size*embedding_size) @ W1 + b1) # (blah, 100)\n",
    "\n",
    "\tlogits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "\tloss = F.cross_entropy(logits, Y) # Does the epic normalisation stuff\n",
    "\tprint(f'{loss=}')\n",
    "\treturn loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8a31c3d3c58cfe49314f156da5c5377d35db16854b5a4a4800d645f76ea86e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
